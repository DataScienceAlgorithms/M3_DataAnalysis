{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 322]() Data Science Algorithms\n",
    "[Gonzaga University](https://www.gonzaga.edu/) |\n",
    "[Sophina Luitel](https://www.gonzaga.edu/school-of-engineering-applied-science/faculty/detail/sophina-luitel-phd-0dba6a9d)\n",
    "\n",
    "---\n",
    "\n",
    "# Data Preparation\n",
    "What are our learning objectives for this lesson?\n",
    "* Learn about the steps involved in data preprocessing\n",
    "* Learn about different attribute types\n",
    "  \n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Gina Sprint's Data Science Algorithms notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Today\n",
    "* Announcements\n",
    "    * LA4 notecard/quiz on next class\n",
    "    * PA2 is posted and is due on Tuesday 9/23.\n",
    "    * Go over PA2\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "The goal of data preprocessing is to produce high-quality data to improve mining results and efficiency\n",
    "\n",
    "At a high level, data preprocessing includes the following steps (these steps are done in any order and often multiple times):\n",
    "1. Data Exploration (basic understanding of meaning, attributes, values, issues)\n",
    "2. Data Reduction (reduce size via aggregation, redundant features, etc.)\n",
    "3. Data Integration (merge/combine multiple datasets)\n",
    "4. Data Cleaning (remove noise and inconsistencies)\n",
    "5. Data Transformation (normalize/scale, etc.)\n",
    "\n",
    "It is important for data mining that your process is transparent and repeatable:\n",
    "* Can repeat \"experiment\" and get the same result\n",
    "* No \"magic\" steps\n",
    "\n",
    "It is important, however, to write down steps (log):\n",
    "* Ideally, someone should be able to take your data, program, and description of steps, rerun everything, and get the same results!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Start by getting to know your dataset. Below are some of the most widely used and general-purpose steps for exploring data:\n",
    "\n",
    "* Inspect structure           \n",
    "* Check for missing/duplicate/invalid values\n",
    "* Understand feature types    \n",
    "* Explore distribution         \n",
    "* Identify outliers            \n",
    "* Examine feature relationships \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on Attributes\n",
    "Different aspects of attributes (variables)\n",
    "* Data (storage) type - e.g., int versus float versus string\n",
    "* Measurement scales - are values discrete or continuous\n",
    "* Semantic type – what the values represent (e.g., colors, ages)\n",
    "\n",
    "### Measurement Scales\n",
    "1. Nominal\n",
    "    * Discrete values without inherent order\n",
    "    * E.g., colors (red, blue, green), identifiers, occupation, gender\n",
    "    * Often ints or strings (but could be any data type)\n",
    "2. Ordinal\n",
    "    * Discrete values with inherent order\n",
    "    * E.g., t-shirt size (s, m, l, xl), grades (A+, A-, B+, ...)\n",
    "    * No guarantee that the difference between values is same\n",
    "    * Often ints or strings (but could be any data type)\n",
    "3. Interval\n",
    "    * Values measured on a scale of equal-sized widths\n",
    "    * Unlike ordinal, can compare and quantify difference between values\n",
    "    * No inherent zero point (i.e., absence)\n",
    "    * Temperature (Celsius, Fahrenheit) is an example\n",
    "4. Ratio\n",
    "    * Interval values with an inherent zero point\n",
    "    * Temperature in Kelvin is an example\n",
    "    * Also counts of things (where 0 means not present)\n",
    "    \n",
    "### Categorical vs Continuous\n",
    "* Categorical roughly means the nominal and ordinal values\n",
    "* Continuous roughly means the rest (interval, ratio) ... aka \"numerical\"\n",
    "* For many algorithms/approaches, this is enough detail\n",
    "\n",
    "### Labeled vs Unlabeled Data\n",
    "* Labeled data implies an attribute that classifies instances (e.g., mpg)\n",
    "    * Goal is typically to predict the class for new instances\n",
    "    * This is called \"Supervised Learning\"\n",
    "* Unlabeled means there isn't such an attribute (for mining purposes)\n",
    "    * Can still find patterns, associations, etc.\n",
    "    * Generally referred to as \"Unsupervised Learning\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Integration\n",
    "Data integration is the process of combining data from multiple sources such as databases, files, APIs, or sensors into a single and consistent format. This makes it easier to analyze the data and use it in data science models or algorithms. It involves cleaning, matching, and aligning the data so everything fits together properly.\n",
    "\n",
    "### Tabular Data\n",
    "Our focus is “Tabular” Data Relational or Structured\n",
    "* Data is organized into tables (rows and columns)\n",
    "\n",
    "Age |Gender |Impressions |Clicks |SignedIn\n",
    "-|-|-|-|-|\n",
    "59 |1 |4 |0 |1\n",
    "19 |0 |5 |0 |1\n",
    "44 |1 |5 |0 |1\n",
    "28 |1 |4 |0 |1\n",
    "61 |1 |10 |1 |1\n",
    "0 |0 |3 |1 |0\n",
    "\n",
    "* Each row is an \"instance\"\n",
    "    * \"example\", \"record\", or \"object\"\n",
    "* Each column is an “attribute” (of the instance)\n",
    "    * \"variables\" or \"fields\"\n",
    "* A \"dataset\" is a (sample) set of instances\n",
    "    * from the \"universe of objects\" (universe of instances)\n",
    "\n",
    "This is a sample of (simulated) daily website click stream data (Example from \"Doing Data Science\", Schutt and O’Neil)\n",
    "* Each row contains attribute values for one user\n",
    "* User’s age, gender (0=female, 1=male), ads shown, ads clicked, and if\n",
    "logged in (0=no, 1=yes)\n",
    "\n",
    "### Keys\n",
    "A \"key\" is one or more attributes that uniquely identifies each row (instance) in a table.\n",
    "\n",
    "\n",
    "For example:\n",
    "\n",
    "UserId |Age |Gender |Impressions |Clicks |SignedIn\n",
    "-|-|-|-|-|-|\n",
    "20 |59 |1 |4 |0 |1\n",
    "15 |19 |0 |5 |0 |1\n",
    "31 |44 |1 |5 |0 |1\n",
    "71 |28 |1 |4 |0 |1\n",
    "51 |61 |1 |10 |1 |1\n",
    "60 |0 |0 |3 |1 |0\n",
    "\n",
    "* here, each UserId value identifies the user\n",
    "\n",
    "Q: What would be the key if the table does not have a UserId column?\n",
    "\n",
    "* There is no explicit key. However, we can sometimes use a combination of columns (a composite key) to uniquely identify each row.\n",
    "* If no combination works, we could create a new coulm (like a row id) to act as a key. \n",
    "\n",
    "### More on Keys: Multiple Attribute Keys (Composite Keys)\n",
    "Composite key (from [GeeksforGeeks](https://www.geeksforgeeks.org/composite-key-in-sql/)): A composite key is made by the combination of two or more columns in a table that can be used to uniquely identify each row in the table when the columns are combined uniqueness of a row is guaranteed, but when it is taken individually it does not guarantee uniqueness, or it can also be understood as a primary key made by the combination of two or more attributes to uniquely identify every row in a table. \n",
    "\n",
    "CarName |ModelYear |MSRP\n",
    "-|-|-\n",
    "ford pinto |75 |2769\n",
    "toyota corolla |75 |2711\n",
    "ford pinto |76 |3025\n",
    "toyota corolla |76 |2789\n",
    "... |... |...\n",
    "\n",
    "Q: What are the key attributes? ... A: {CarName, ModelYear}\n",
    "\n",
    "Q: Why not just CarName? ... A: Values not unique across rows\n",
    "\n",
    "### More on Keys: Foreign Keys\n",
    "A \"Foreign Key\" is a reference to instances, typically to instances in another table (but could be to the same table)\n",
    "\n",
    "For example:\n",
    "\n",
    "SaleId |EmployeeId |CarName |ModelYear |Amt\n",
    "-|-|-|-|-\n",
    "555 |12 |ford pinto |75 |3076\n",
    "556 |12 |toyota corolla |75 |2611\n",
    "998 |13 |toyota corolla |75 |2800\n",
    "999 |12 |toyota corolla |76 |2989\n",
    "... |... |... |... |...\n",
    "\n",
    "Q: What is the key?\n",
    "* {SaleId}\n",
    "\n",
    "Q: What are the foreign keys (references)?\n",
    "* {EmployeeId} for information about the salesperson\n",
    "* {CarName, ModelYear}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins\n",
    "\n",
    "We can **join** (combine) two tables based on common attributes, typically keys or foreign keys to integrate related data into a unified view.\n",
    "\n",
    "### Join Types\n",
    "\n",
    "- **Inner Join**: Returns only the rows with matching values in both tables.\n",
    "- **Left Outer Join**: Returns all rows from the left table, and matched rows from the right. Missing values from the right become `NA`.\n",
    "- **Right Outer Join**: Returns all rows from the right table, and matched rows from the left. Missing values from the left become `NA`.\n",
    "- **Full Outer Join**: Returns all rows from both tables. If there''s no match, `NA` is used to fill missing fields.\n",
    "\n",
    "![](https://raw.githubusercontent.com/DataScienceAlgorithms/M3_DataAnalysis/main/figures/join.png)\n",
    "\n",
    "\n",
    "Image credit: [W3Schools SQL Join](https://www.w3schools.com/sql/sql_join.asp)\n",
    "\n",
    "---\n",
    "\n",
    "#### Example Tables\n",
    "\n",
    "**Sales Table**\n",
    "\n",
    "| SaleId | EmployeeId | CarName         | ModelYear | Amt  |\n",
    "|--------|------------|------------------|-----------|------|\n",
    "| 555    | 12         | ford pinto       | 75        | 3076 |\n",
    "| 556    | 12         | toyota corolla   | 75        | 2611 |\n",
    "| 998    | 13         | toyota corolla   | 75        | 2800 |\n",
    "| 999    | 12         | toyota corolla   | 76        | 2989 |\n",
    "| ...    | ...        | ...              | ...       | ...  |\n",
    "\n",
    "**MSRP Table**\n",
    "\n",
    "| CarName        | ModelYear | MSRP |\n",
    "|----------------|-----------|------|\n",
    "| ford pinto     | 75        | 2769 |\n",
    "| toyota corolla | 75        | 2711 |\n",
    "| ford pinto     | 76        | 3025 |\n",
    "| toyota corolla | 77        | 2789 |\n",
    "| ...            | ...       | ...  |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### Inner Join Result\n",
    "\n",
    "| SaleId | EmployeeId | CarName         | ModelYear | Amt  | MSRP |\n",
    "|--------|------------|------------------|-----------|------|------|\n",
    "| 555    | 12         | ford pinto       | 75        | 3076 | 2769 |\n",
    "| 556    | 12         | toyota corolla   | 75        | 2611 | 2711 |\n",
    "| 998    | 13         | toyota corolla   | 75        | 2800 | 2711 |\n",
    "\n",
    "---\n",
    "\n",
    "### Full Outer Join Result\n",
    "\n",
    "| SaleId | EmployeeId | CarName         | ModelYear | Amt   | MSRP |\n",
    "|--------|------------|------------------|-----------|-------|------|\n",
    "| 555    | 12         | ford pinto       | 75        | 3076  | 2769 |\n",
    "| 556    | 12         | toyota corolla   | 75        | 2611  | 2711 |\n",
    "| 998    | 13         | toyota corolla   | 75        | 2800  | 2711 |\n",
    "| 999    | 12         | toyota corolla   | 76        | 2989  | NA   |\n",
    "| NA     | NA         | ford pinto       | 76        | NA    | 3025 |\n",
    "| NA     | NA         | toyota corolla   | 77        | NA    | 2789 |\n",
    "\n",
    ">  **left outer join** = inner join + unmatched left rows  \n",
    ">  **right outer join** = inner join + unmatched right rows\n",
    "\n",
    "---\n",
    "\n",
    "Q: How would we join these two tables?\n",
    "\n",
    "MPG |Cyls |Displacement | Hrspwr | Wght| Accel |ModelYear| Origin |CarName\n",
    "-|-|-|-|-|-|-|-|-\n",
    "23.0 | 4 | 140.0 | 83.0 | 2639 | 17.0 | 75 | 1 | ford pinto\n",
    "29.0 | 4 | 97.0 | 75.0 | 2171 | 16.0 | 75 | 3 | toyota corolla\n",
    "... |... |... |... |... |... |... |... |...\n",
    "\n",
    "CarName|ModelYear |MSRP\n",
    "-|-|-\n",
    "ford pinto |75 |2769\n",
    "toyota corolla |75 |2711\n",
    "... |... |...\n",
    "\n",
    "* Join both on the composite key {CarName, ModelYear}\n",
    "\n",
    "\n",
    "### Join Practice Problem\n",
    "\n",
    "**Task:** Perform both an **inner join** and a **full outer join** on the following tables using the composite key `{CarName, ModelYear}`\n",
    "\n",
    "**Sales Table**\n",
    "\n",
    "| ModelYear | EmployeeId | SaleId | CarName         | Amt   |\n",
    "|-----------|------------|--------|------------------|--------|\n",
    "| 75.0      | 12.0       | 555.0  | ford pinto       | 3076.0 |\n",
    "| 79.0      | 12.0       | 556.0  | toyota truck     | 2989.0 |\n",
    "| 75.0      | 12.0       | 557.0  | toyota corolla   | 2611.0 |\n",
    "| 75.0      | 13.0       | 996.0  | toyota corolla   | 2800.0 |\n",
    "| 76.0      | 12.0       | 997.0  | toyota corolla   | 2989.0 |\n",
    "| 74.0      | 12.0       | 998.0  | ford pinto       | 2989.0 |\n",
    "| 77.0      | 12.0       | 999.0  | ford mustang     | 2989.0 |\n",
    "\n",
    "**MSRP Table**\n",
    "\n",
    "| CarName        | MSRP  | ModelYear |\n",
    "|----------------|--------|-----------|\n",
    "| honda accord   | 2789.0 | 75.0      |\n",
    "| ford pinto     | 2769.0 | 75.0      |\n",
    "| toyota corolla | 2711.0 | 75.0      |\n",
    "| ford pinto     | 3025.0 | 76.0      |\n",
    "| toyota corolla | 2789.0 | 77.0      |\n",
    "| range rover    | 3333.0 | 70.0      |\n",
    "| ford pinto     | 2567.0 | 73.0      |\n",
    "| toyota corolla | 2999.0 | 75.0      |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### Inner Join Solution\n",
    "\n",
    "| ModelYear | EmployeeId | SaleId | CarName        | Amt   | MSRP  |\n",
    "|-----------|------------|--------|----------------|--------|--------|\n",
    "| 75.0      | 12.0       | 555.0  | ford pinto     | 3076.0 | 2769.0 |\n",
    "| 75.0      | 12.0       | 557.0  | toyota corolla | 2611.0 | 2711.0 |\n",
    "| 75.0      | 12.0       | 557.0  | toyota corolla | 2611.0 | 2999.0 |\n",
    "| 75.0      | 13.0       | 996.0  | toyota corolla | 2800.0 | 2711.0 |\n",
    "| 75.0      | 13.0       | 996.0  | toyota corolla | 2800.0 | 2999.0 |\n",
    "\n",
    "### Full Outer Join Solution\n",
    "\n",
    "| ModelYear | EmployeeId | SaleId | CarName        | Amt   | MSRP  |\n",
    "|-----------|------------|--------|----------------|--------|--------|\n",
    "| 75.0      | 12.0       | 555.0  | ford pinto     | 3076.0 | 2769.0 |\n",
    "| 79.0      | 12.0       | 556.0  | toyota truck   | 2989.0 | NA     |\n",
    "| 75.0      | 12.0       | 557.0  | toyota corolla | 2611.0 | 2711.0 |\n",
    "| 75.0      | 12.0       | 557.0  | toyota corolla | 2611.0 | 2999.0 |\n",
    "| 75.0      | 13.0       | 996.0  | toyota corolla | 2800.0 | 2711.0 |\n",
    "| 75.0      | 13.0       | 996.0  | toyota corolla | 2800.0 | 2999.0 |\n",
    "| 76.0      | 12.0       | 997.0  | toyota corolla | 2989.0 | NA     |\n",
    "| 74.0      | 12.0       | 998.0  | ford pinto     | 2989.0 | NA     |\n",
    "| 77.0      | 12.0       | 999.0  | ford mustang   | 2989.0 | NA     |\n",
    "| 75.0      | NA         | NA     | honda accord   | NA     | 2789.0 |\n",
    "| 76.0      | NA         | NA     | ford pinto     | NA     | 3025.0 |\n",
    "| 77.0      | NA         | NA     | toyota corolla | NA     | 2789.0 |\n",
    "| 70.0      | NA         | NA     | range rover    | NA     | 3333.0 |\n",
    "| 73.0      | NA         | NA     | ford pinto     | NA     | 2567.0 |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning  \n",
    "\n",
    "Data cleaning is the process of detecting, correcting, or removing inaccurate, inconsistent, or incomplete records from a dataset to ensure it is accurate, reliable, and ready for analysis or modeling.  \n",
    "\n",
    "\n",
    "## Common Data Quality Issues & Techniques  \n",
    "\n",
    "### 1. Noisy vs Invalid Values  \n",
    "- **Noisy Values**: Data that is *valid* but recorded incorrectly or affected by random error.  \n",
    "  - Examples:  \n",
    "    - Decimal error: `5.72` instead of `57.2`  \n",
    "    - Wrong categorical entry\n",
    "  - Fix:  \n",
    "    - Use smoothing techniques (e.g., binning, moving average, regression).  \n",
    "    - Correct based on context (e.g., rounding, reference dictionaries).  \n",
    "\n",
    "- **Invalid Values**: Values that fall *outside the domain of acceptable inputs*.  \n",
    "  - Examples:  \n",
    "    - Rating = `6` on a 1–5 scale  \n",
    "    - Age = `-10`  \n",
    "    - Category = `57.2X` (nonsensical entry)  \n",
    "  - Fix:  \n",
    "    - Apply range/domain validation.  \n",
    "    - Remove or replace invalid values with corrected/estimated ones.  \n",
    "\n",
    "\n",
    "### 2. Missing Values  \n",
    "-  skipped fields, data entry errors, system failure, merging mismatched sources.  \n",
    "- Fix: \n",
    "  - **Deletion:** drop rows/columns with too many missing entries.  \n",
    "  - **Replacement:**  \n",
    "    - Manual entry (if information available).  \n",
    "    - Constant value (e.g., “Unknown”).  \n",
    "    - Central tendency measure (mean, median, mode).  \n",
    "  - **Prediction:** estimate using regression or classifiers.  \n",
    "  - **Context-based replacement:** e.g., average price *based on model year*.  \n",
    "\n",
    "\n",
    "### 3. Outliers  \n",
    "- Values that deviate significantly from the rest of the data.  \n",
    "- Caused by errors, rare events, or true anomalies.  \n",
    "- Fix:\n",
    "  - Verify whether the outlier is an error or valid.  \n",
    "  - Remove, cap, or transform values depending on context.  \n",
    "\n",
    "\n",
    "### 4. Deduplication  \n",
    "- *Process of removing duplicate records caused by integration issues, human entry mistakes, or system glitches.  \n",
    "- Fix:  \n",
    "  - Detect duplicates using IDs, keys, or fuzzy matching.  \n",
    "  - Keep only one unique record.  \n",
    "\n",
    "\n",
    "### 5. Standardization  \n",
    "- Making sure data follows a consistent format.  \n",
    "- Examples:  \n",
    "  - Dates: `09-18-2025` vs `2025/09/18`.  \n",
    "  - Categorical labels: `NY`, `New York`, `N.Y.`.  \n",
    "- Fix: Convert all values to a standard, unified format.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q: What are the issues and how to fix it?\n",
    "\n",
    "| PassengerID | Name            | Age  | Gender | Pclass | Fare  |\n",
    "| ----------- | --------------- | ---- | ------ | ------ | ----- |\n",
    "| 1           | John Smith      | 22   | Male   | 3      | 7.25  |\n",
    "| 2           | Mary Johnson    |      | Female | 1      | 71.83 |\n",
    "| 3           | William Brown   | 350  | male   | 3      | 8.05  |\n",
    "| 4           | Elizabeth Lee   | None | Female | 1      | 531   |\n",
    "| 5           | Charles Davis   | 28   | Male   | 1      |       |\n",
    "| 6           | Anna Wilson     | 19   | male   |        | 7.9   |\n",
    "| 7           | George Miller   |      | Male   | 3      | 8.5   |\n",
    "| 8           | Patricia Taylor | 40   |        | 2      | 21.0  |\n",
    "| 9           | Mark White      | 17   | Male   | 3      | 5000  |\n",
    "| 10          | Laura Adams     | -2   | Female | 1      | 30.0  |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Summary Statistics  \n",
    "\n",
    "Summary statistics give initial insights into a dataset. They describe key properties such as size, center, and spread.  \n",
    "\n",
    "\n",
    "\n",
    "### 1. Number of Instances  \n",
    "- How many rows are in the dataset?  \n",
    "\n",
    "\n",
    "### 2. Min and Max Values  \n",
    "- Show the smallest and largest values for each attribute.  \n",
    "- Makes sense for:  \n",
    "  - Ordinal, Interval, Ratio data (ordered).  \n",
    "  - Not meaningful for Nominal (unordered categories).  \n",
    "- For Nominal data: count frequencies of each category.  \n",
    "\n",
    "- Null values (NA):  \n",
    "    - Represent “unknown/undefined.”  \n",
    "    - Often ignored in quick summaries, but should be considered for cleaning (imputation, removal, or flagging).  \n",
    "\n",
    "### 3. Central Tendency (Middle Values)  \n",
    "\n",
    "- Midrange: `(max + min) / 2` (rarely used in practice).  \n",
    "- **Mean** (Average):  \n",
    "  - $\\bar{x} = \\frac{(x_1 + x_2 + ... + x_n)}{n}$  \n",
    "  - Python: `sum(column) / float(len(column))`  \n",
    "  - Sensitive to outliers  \n",
    "  - Meaningful for Interval/Ratio** only  \n",
    "- **Median**:  \n",
    "  - Middle value in sorted list.  \n",
    "  - Robust to outliers.  \n",
    "  - Works for Ordinal/Interval/Ratio data.  \n",
    "- **Mode**:  \n",
    "  - Most frequent value.  \n",
    "  - Works for all data types, including Nominal.  \n",
    "\n",
    "\n",
    "### 4. Dispersion (Spread of Data)  \n",
    "\n",
    "- **Range:** max – min  \n",
    "- **Quantiles**: Divide sorted data into (roughly) equal-sized groups  \n",
    "  * **2-quantiles** = Median (splits data into 2 halves)  \n",
    "  * **Quartiles** = 3 cut points dividing data into 4 groups (Q1, Q2, Q3)  \n",
    "    * Q1 = 25th percentile  \n",
    "    * Q2 = 50th percentile (median)  \n",
    "    * Q3 = 75th percentile  \n",
    "    * Used in **box plots**  \n",
    "  * **Interquartile Range (IQR)** = Q3 – Q1  \n",
    "    * Spread of the \"middle 50%\" of data  \n",
    "  * **Percentiles** = 100-quantiles  \n",
    "    * Example: 90th percentile = value below which 90% of data falls  \n",
    "- **Variance & Standard Deviation:**  \n",
    "  - Variance = average squared distance from the mean  \n",
    "  - Formula:  \n",
    "    $$\\text{Var}(X) = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}{n}$$  \n",
    "  - Standard Deviation = $\\sqrt{\\text{Var}(X)}$  \n",
    "  - Python: `numpy.std(vals)`\n",
    "  - * ... more on numpy later\n",
    "\n",
    "**Normal distribution (68–95–99.7 Rule):**  \n",
    "- 68% of values within 1 SD of mean  \n",
    "- 95% within 2 SD  \n",
    "- 99.7% within 3 SD  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pratice Question \n",
    "\n",
    "You are provided a dataset.\n",
    "\n",
    "Some values may be:  \n",
    "\n",
    "- Missing\n",
    "- Invalid\n",
    "- Outliers \n",
    "- Noisy\n",
    "\n",
    "## Task\n",
    "\n",
    "1. Identify invalid, missing, noisy, or outlier values in each column.  \n",
    "2. Clean / Impute the data appropriately:  \n",
    "   - Replace missing or invalid Age values with the average of valid ages.  \n",
    "   - Standardize Gender values.  \n",
    "   - Cap Score outliers to a reasonable maximum (e.g., 100).  \n",
    "3. **Compute**:  \n",
    "   - Average Age (`avg_age`)  \n",
    "   - Average Score (`avg_score`)  \n",
    "   - **Secret code = avg_age + avg_score**  \n",
    "4. Print the first 5 cleaned rows and the secret code.\n",
    "\n",
    "<!---\n",
    "import random\n",
    "\n",
    "data = []\n",
    "header=['ID','Age','Gender','Score']\n",
    "data.append(header)\n",
    "for i in range(1,50):\n",
    "    age = random.choice([20, 21, 22, -1, None])\n",
    "    gender = random.choice([\"Male\", \"male\", \"Meel\", \"Female\", \"Fmale\"])\n",
    "    score = random.choice([78, 85, 90, 250, 95])  \n",
    "    data.append([i, age, gender, grade, score])\n",
    "\n",
    "\n",
    "\n",
    "#  Clean Age\n",
    "valid_ages = [row[1] for row in data[1:] if row[1] is not None and row[1] >= 0]\n",
    "avg_valid_age = sum(valid_ages) / len(valid_ages)\n",
    "for row in data[1:]:\n",
    "    if row[1] is None or row[1] < 0:\n",
    "        row[1] = round(avg_valid_age)\n",
    "\n",
    "# Clean Gender\n",
    "for row in data[1:]:\n",
    "    g = row[2].lower().replace(\" \", \"\")\n",
    "    if g == \"male\" or g == \"meel\":\n",
    "        row[2] = \"Male\"\n",
    "    elif g == \"fmale\":\n",
    "        row[2] = \"Female\"\n",
    "    else:\n",
    "        row[2] = row[2].capitalize()\n",
    "\n",
    "\n",
    "# Clean Score\n",
    "for row in data[1:]:\n",
    "    if row[4] > 100:\n",
    "        row[4] = 95  # Cap outlier\n",
    "\n",
    "# Compute averages\n",
    "avg_age = sum(row[1] for row in data[1:]) / len(data)\n",
    "avg_score = sum(row[4] for row in data[1:]) / len(data)\n",
    "\n",
    "# Secret Code\n",
    "secret_code = avg_age + avg_score\n",
    "\n",
    "# Step 7: Print first 5 cleaned rows + secret code\n",
    "print(\"First 5 rows of Cleaned Dataset:\")\n",
    "\n",
    "for row in data[:20]:\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nAverage Age:\", avg_age)\n",
    "print(\"Average Score:\", avg_score)\n",
    "print(\"Secret Code (Age + Score):\", secret_code)\n",
    "\n",
    "!>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "data = []\n",
    "header=['ID','Age','Gender','Score']\n",
    "data.append(header)\n",
    "for i in range(1,50):\n",
    "    age = random.choice([20, 21, 22, -1, None])\n",
    "    gender = random.choice([\"Male\", \"male\", \"Meel\", \"Female\", \"Fmale\"])\n",
    "    score = random.choice([78, 85, 90, 250, 95])  \n",
    "    data.append([i, age, gender, score])\n",
    "\n",
    "\n",
    "\n",
    "# Clean Age\n",
    "\n",
    "# Clean Gender\n",
    "\n",
    "# Clean Score\n",
    "\n",
    "\n",
    "# Compute averages\n",
    "\n",
    "\n",
    "# Compute Secret Code\n",
    "\n",
    "\n",
    "#Print first 5 cleaned rows and secret code\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
