{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 322]() Data Science Algorithms\n",
    "[Gonzaga University](https://www.gonzaga.edu/) |\n",
    "[Sophina Luitel](https://www.gonzaga.edu/school-of-engineering-applied-science/faculty/detail/sophina-luitel-phd-0dba6a9d)\n",
    "\n",
    "---\n",
    "\n",
    "# Data Transformation\n",
    "What are our learning objectives for this lesson?\n",
    "* Understand what data transformation is and why it’s important.\n",
    "* Apply scaling, normalization, encoding, and discretization.\n",
    "\n",
    "\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Gina Sprints's Notes from Data Science Algorithm, Fall 2024\n",
    "* [Data Mining Concepts and Techniues](https://theswissbay.ch/pdf/Gentoomen%20Library/Data%20Mining/Data%20Mining%20Concepts%20And%20Techniques_Jiawei%20Han-Micheline%20Kamber%20%282000%29.pdf)\n",
    "\n",
    "## Today\n",
    "* Announcement\n",
    "    * PA2 is due tomorrow \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation \n",
    "\n",
    "Data transformation is the process of converting raw data into a more suitable form for analysis and modeling. Many datasets have features with different scales, units, or distributions, and directly using them in analysis or machine learning can lead to biased or misleading results. Transformation ensures that features are comparable, consistent, and ready for modeling.\n",
    "\n",
    "## Transformation Techniques\n",
    "\n",
    "**1.  Scaling**\n",
    "\n",
    "In data science, numeric features often have different ranges or units, which can affect analysis or machine learning algorithms. Scaling is the general process of adjusting the magnitude of feature values.\n",
    "\n",
    "**a. Normalization (Min-Max Scaling)**  \n",
    "- Rescale a numeric feature to a fixed range, usually [0,1].  \n",
    "- Makes it easier to **compare and combine features** that have different units or magnitudes.\n",
    "\n",
    "\n",
    "$$\n",
    "v' = \\frac{v - \\text{min}_A}{\\text{max}_A - \\text{min}_A}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- v = original value    \n",
    "- $min_A$ = minimum value of the feature  \n",
    "- $max_A$ = maximum value of the feature  \n",
    "- $v'$ = scaled value in [0,1]  \n",
    "\n",
    "- Example (Auto dataset):  \n",
    "  - `Weight` = 3500 pounds  \n",
    "  - `MPG` = 25 miles per gallon  \n",
    "- Without scaling, it is hard to **compare or combine these numbers**.\n",
    "\n",
    "\n",
    "| Car      | Weight | MPG | Scaled Weight | Scaled MPG |\n",
    "|----------|--------|-----|---------------|------------|\n",
    "| Car A    | 3500   | 25  | 0.75          | 0.5        |\n",
    "| Car B    | 2000   | 40  | 0.0           | 1.0        |\n",
    "| Car C    | 5000   | 15  | 1.0           | 0.25       |\n",
    "| Car D    | 4000   | 35  | 0.833         | 0.833      |\n",
    "\n",
    "\n",
    "- Both features are now in the same scale [0,1], easier to handle and compare.\n",
    "\n",
    "\n",
    "\n",
    "**b. Standardization (Z-Score)**  \n",
    "- Z-score normalization centers data around mean = 0 and scales it to standard deviation = 1. \n",
    "- Useful when features have different magnitudes and you want to retain outlier effects.\n",
    "- Z-Score is less sensitive to outliers compared to Min–Max scaling.\n",
    "\n",
    "\n",
    "$$\n",
    "v' = \\frac{v - \\bar{X}}{\\sigma_X}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- v = original value  \n",
    "- $\\bar{X}$ = mean of the feature  \n",
    "- $\\sigma_X$ = standard deviation of the feature  \n",
    "- v' = normalized value  \n",
    "\n",
    "\n",
    "Suppose the Age column has:  \n",
    "- Mean = 30  \n",
    "- Standard deviation = 10  \n",
    "- Value v = 35  \n",
    "\n",
    "$$\n",
    "v' = \\frac{35 - 30}{10} = 0.5\n",
    "$$\n",
    "\n",
    "Another value, v = 20:  \n",
    "$$\n",
    "v' = \\frac{20 - 30}{10} = -1.0\n",
    "$$\n",
    "\n",
    "- Now all values are expressed in standard deviations from the mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretization\n",
    "\n",
    "Discretization converts numeric (continuous) attributes into discrete (categorical) values.  \n",
    "Reasons to apply discretization:\n",
    "- To create frequency diagrams (e.g., histograms)  \n",
    "  - Example: `plt.hist(xs, bins=20)`  \n",
    "- To explore relationships with another discrete attribute  \n",
    "- To remove noise from data  \n",
    "- To prepare data for specific mining or machine learning algorithms\n",
    "\n",
    "\n",
    "## Binning Techniques\n",
    "\n",
    "1. Domain-specific (Real, Meaningful Bins)\n",
    "- Define bins based on real-world knowledge or standards \n",
    "- Example: US DOE fuel economy ratings:\n",
    "\n",
    "| Rating | MPG      |\n",
    "|--------|----------|\n",
    "| 10     | ≥ 45     |\n",
    "| 9      | 37–44    |\n",
    "| 8      | 31–36    |\n",
    "| 7      | 27–30    |\n",
    "| 6      | 24–26    |\n",
    "| 5      | 20–23    |\n",
    "| 4      | 17–19    |\n",
    "| 3      | 15–16    |\n",
    "| 2      | 14       |\n",
    "| 1      | ≤ 13     |\n",
    "\n",
    "- The values that define bins are called **“cut points”**   \n",
    "\n",
    "\n",
    "\n",
    "2. Equal-Width Binning\n",
    "- Divide the range of values into n equal-width intervals \n",
    "- Example: Age 18–55, n=3 bins → 18–30, 30–43, 43–55\n",
    "\n",
    "Values: 18, 22, 25, 28, 30, 35, 40, 45, 50, 55  \n",
    "\n",
    "- Width= (55-18)/ 3 = 12.333\n",
    "- 3 equal-width bins:  \n",
    "  - Bin 1: 18.00–30.333 → 18, 22, 25, 28,30  \n",
    "  - Bin 2: 30.333–42.666 → 35, 40  \n",
    "  - Bin 3: 42.666-55 → 45, 50, 55  \n",
    "\n",
    "**Notes on Cut Points**\n",
    "- For n bins, we assume N + 1 cut point (we will use this approach becuase numpy's histogram() function uses this approach).\n",
    "- includes min and max\n",
    "- Half-open convention: All bins except the last are `[start, end)`, last bin includes max `[start, end]`.  \n",
    "- Alternative conventions exist:\n",
    "  - All bins half-open, include max separately  \n",
    "  - Use N cut points or N-1 cut points depending on implementation  \n",
    "- **Key point:** Cut points ensure all values, including min and max, are properly covered.\n",
    "\n",
    "\n",
    "3. Equal-Frequency Binning\n",
    "- Divide the data so that each bin has roughly the same number of observations \n",
    "- Example: 15 student ages → 3 bins with 5 students each  \n",
    "- Does not consider actual distribution of values  \n",
    "\n",
    "\n",
    "4. Cluster-Based Binning\n",
    "- Use clustering to group similar values together  \n",
    "- Example: partition numeric values into bins based on closeness of data points\n",
    "\n",
    "5. Classification-Based Binning\n",
    "- Find cut points that create ranges with instances having the most number of the same classes as possible  \n",
    "\n",
    "\n",
    "## Encoding  \n",
    "\n",
    "- Convert categorical features into numeric form for analysis or machine learning.  \n",
    "- Most algorithms require numeric input.\n",
    "\n",
    "\n",
    "\n",
    "### 1. Label Encoding\n",
    "- Assigns a unique integer to each category.  \n",
    "- Can be 0,1,2,… or 1,2,3,…  \n",
    "- Best for ordinal features where order matters.\n",
    "\n",
    "**Example: Education Level (Ordinal)**\n",
    "| Education Level | Encoded Value |\n",
    "|-----------------|---------------|\n",
    "| High School     | 1             |\n",
    "| Bachelor        | 2             |\n",
    "| Master          | 3             |\n",
    "| PhD             | 4             |\n",
    "\n",
    " \n",
    " Implies order: High School < Bachelor < Master < PhD\n",
    "\n",
    "\n",
    "\n",
    "### 2. One-Hot Encoding\n",
    "- Creates a binary column for each category.  \n",
    "- Best for nominal features with no natural order.\n",
    "\n",
    "**Example: Color (Nominal)**\n",
    "| Color   | Red | Green | Blue |\n",
    "|---------|-----|-------|------|\n",
    "| Red     | 1   | 0     | 0    |\n",
    "| Green   | 0   | 1     | 0    |\n",
    "| Blue    | 0   | 0     | 1    |\n",
    "| Red     | 1   | 0     | 0    |\n",
    "\n",
    "No order is implied among categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretization Practice\n",
    "1. Given a list of values and the number of equal-width bins to create (N), write a function to return a list of the N + 1 cutoff points.\n",
    "1. Given a list of values and a list of N + 1 cutoff points, write a function to return the corresponding frequencies of the N bins.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
